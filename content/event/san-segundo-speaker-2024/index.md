+++
title = "Speaker identification of artificially cloned voices versus the voices of identical twins: a perceptual test."
date = 2024-09-11T20:07:47+02:00  # Schedule page publish date.
draft = false

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
all_day = true

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = ["Eugenia San Segundo", "Mark Gibson"]

# Abstract and optional shortened version.
abstract = "Previous perceptual studies on voice deepfakes are mostly online tests asking listeners to decide whether a voice is real or fake. In this study, we designed a multiple forced choice (same / different) listening experiment with Praat using 12 different voice samples, each around 3-5 seconds long, extracted from semi-directed spontaneous conversations of two pairs of twins. There are two different voice samples (a and b) per twin member (A and B), and each voice was also artificially cloned. The experiment was set up in Praat with 20 stimuli in 3 types of pairings: 8 same-speaker pairings, 8 different-speaker (intra-twin) pairings, and 4 combinations of a speaker with his voice deepfake.<br> 30 listeners took part in the perceptual experiment under controlled conditions (same headphones, computer, and silent room). Listeners were college students, native speakers of Standard Peninsular Spanish. They were not informed that the stimuli included twin voices and deepfakes. The test duration was approximately 10 minutes, and reaction times were measured. <br> The objective of this investigation was to determine whether identification results (hits, misses, false alarms, and correct rejections), as well as reaction times, depend on the type of stimuli combination: same-speaker, different-speaker, or real-deepfake combinations. Our hypotheses are: (1) listeners will perform worse when listening to deepfakes than when listening to real voices, even if real voices are from identical twins, who are extreme cases of similarity in humans; (2) reaction times will be longer when listening to deepfakes."
abstract_short = ""

# Name of event and optional event URL.
event = "ACÚSTICA2024 – XIII Congreso Ibérico de Acústica – 55º Congreso Español de Acústica – TECNIACÚSTICA® 2024"
event_url = "http://www.spacustica.pt/acustica2024/index.html"

# Location of event.
location = "Faro (Portugal)"

# Is this a featured talk? (true/false)
featured = true

# Projects (optional).
#   Associate this talk with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["deep-learning"]` references 
#   `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects = []

# Slides (optional).
#   Associate this page with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references 
#   `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides = ""

# Tags (optional).
#   Set `tags = []` for no tags, or use the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = []

# Links (optional).
url_pdf = ""
url_slides = ""
url_video = ""
url_code = ""

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
[image]
  # Caption (optional)
  caption = ""

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = ""
+++
